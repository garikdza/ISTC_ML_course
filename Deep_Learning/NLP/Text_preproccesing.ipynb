{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preproccesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After a text is obtained, we start with text preproccesing. Text preproccesing includes:\n",
    "* converting all letters to lower or upper case\n",
    "* converting numbers into words or removing numbers\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing white spaces\n",
    "* expanding abbreviations\n",
    "* removing stop words, sparse terms, and particular words\n",
    "* text canonicalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
    "result = re.sub(r\"\\d+\", \"\", input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\n",
    "result = s = re.sub(r'[^\\w\\s]','',input_str)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t a string example\t \n",
      "a string example\n"
     ]
    }
   ],
   "source": [
    "input_str = \"\\t a string example\\t \"\n",
    "print(input_str)\n",
    "input_str = input_str.strip()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. In this table (“Tokenization” sheet) several tools for implementing tokenization are described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/Tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK), a suite of libraries and programs for symbolic and statistical natural language processing.\n",
    "Stop words cleaning also supports many modern libraries, like Sklearn, SpaCy, Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming \n",
    "Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words [14]) and Lancaster stemming algorithm (a more aggressive stemming algorithm). In the “Stemming” sheet of the table some stemmers are described.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "input_str=\"There are several types of stemming algorithms.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words.\n",
    "<br>Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n",
      "\n",
      "\n",
      "There\n",
      "are\n",
      "several\n",
      "type\n",
      "of\n",
      "stemming\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "input_str=\"been had done languages cities mice\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))\n",
    "    \n",
    "print(\"\\n\")\n",
    "input_str1 = \"There are several types of stemming algorithms.\"\n",
    "\n",
    "input_str=word_tokenize(input_str1)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learn ( ml ) scientific study algorithm statistical model computer system use order perform specific task effectively without use explicit instruction , rely pattern inference instead . see subset artificial intelligence . machine learn algorithm build mathematical model base sample datum , know train datum , order make prediction decision without explicitly programme perform task.[1][2]:2 machine learn algorithm use wide variety application , email filter , computer vision , infeasible develop algorithm specific instruction perform task . machine learn closely relate computational statistic , focus make prediction use computer . study mathematical optimization deliver method , theory application domain field machine learn . datum mine field study within machine learn , focus exploratory datum analysis unsupervised learning.[3][4 ] application across business problem , machine learn also refer predictive analytics .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy #load spacy\n",
    "# dowenload spacy \"english\" model with python -m spacy download en\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "text = \"\"\"Machine learning (ML) is the scientific study of algorithms and statistical models that\n",
    "        computer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to perform the task.[1][2]:2 Machine learning algorithms are used in a wide variety of applications, \n",
    "        such as email filtering, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task.\n",
    "        Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.[3][4] In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\"\n",
    "normalize(text,lowercase=True,remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re tutorial: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285\n",
    "<br>stemming vs lemmatization: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "<br> more:https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "<br> keras text preproccesing:https://keras.io/preprocessing/text/\n",
    "<br> intro to spacy:https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
